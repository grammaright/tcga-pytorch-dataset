{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import requests\n",
    "import json\n",
    "\n",
    "#gene_list = list(pd.read_csv(\"./pam50.txt\",sep=\"\\t\", header = None, index_col = False)[0])\n",
    "gene_list = list(pd.read_excel(\"./pan_cancer.xlsx\", header = None, index_col = False)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_excel(\"./pan_cancer.xlsx\", header = None, index_col = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gene_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\n",
    "    \"file_name\",\n",
    "    \"cases.submitter_id\",\n",
    "    \"cases.samples.sample_type\",\n",
    "    \"cases.disease_type\",\n",
    "    \"file_id\"\n",
    "    ]\n",
    "\n",
    "fields = \",\".join(fields)\n",
    "\n",
    "files_endpt = \"https://api.gdc.cancer.gov/legacy/files\"\n",
    "# This set of filters is nested under an 'and' operator.\n",
    "filters = {\n",
    "    \"op\": \"and\",\n",
    "    \"content\":[\n",
    "        {\n",
    "        \"op\": \"in\",\n",
    "        \"content\":{\n",
    "            \"field\": \"cases.project.program.name\",\n",
    "            \"value\": [\"TCGA\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "        \"op\": \"in\",\n",
    "        \"content\":{\n",
    "            \"field\": \"cases.project.primary_site\",\n",
    "            \"value\": [\"Breast\", \"Lung\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "        \"op\": \"in\",\n",
    "        \"content\":{\n",
    "            \"field\": \"files.data_category\",\n",
    "            \"value\": [\"Gene expression\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "        \"op\": \"in\",\n",
    "        \"content\":{\n",
    "            \"field\": \"files.data_type\",\n",
    "            \"value\": [\"Gene expression quantification\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "        \"op\": \"in\",\n",
    "        \"content\":{\n",
    "            \"field\": \"files.experimental_strategy\",\n",
    "            \"value\": [\"RNA-Seq\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "        \"op\": \"in\",\n",
    "        \"content\":{\n",
    "            \"field\": \"files.data_format\",\n",
    "            \"value\": [\"TXT\"]\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# A POST is used, so the filter parameters can be passed directly as a Dict object.\n",
    "\n",
    "params = {\n",
    "    \"filters\": filters,\n",
    "    \"fields\": fields,\n",
    "    \"format\": \"TSV\",\n",
    "    \"size\": \"20000\"\n",
    "    }\n",
    "\n",
    "# The parameters are passed to 'json' rather than 'params' in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "def manifest_loader(fields, filters,label):\n",
    "    assert label in fields\n",
    "    params = {\n",
    "    \"filters\": filters,\n",
    "    \"fields\": fields,\n",
    "    \"format\": \"TSV\",\n",
    "    \"size\": \"20000\"\n",
    "    }\n",
    "\n",
    "    response = requests.post(files_endpt, headers = {\"Content-Type\": \"application/json\"}, json = params)\n",
    "    manifest = response.content\n",
    "\n",
    "    data = StringIO(str(response.content,'utf-8')) \n",
    "    manifest=pd.read_csv(data, sep = '\\t', index_col = False)\n",
    "    \n",
    "    return manifest, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "\n",
    "# 로그의 출력 기준 설정\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# log 출력 형식\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# log 출력\n",
    "\n",
    "# log를 파일에 출력\n",
    "file_handler = logging.FileHandler('analysis.log')\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, logging\n",
    "\n",
    "logging.basicConfig(filename='analysis.log', encoding='utf-8', level=logging.DEBUG)\n",
    "\n",
    "class GeneExpressionLoader(IterableDataset):\n",
    "    def __init__(self,raw_data, label_column, gene_list, normalized = False ):\n",
    "        super(GeneExpressionLoader).__init__()\n",
    "        #raw_data, label_column = manifest_loader(fields, filters,label)\n",
    "        ## 지정해준 fields, filter에 해당하는 파일들의 목록(manifest) 를 불러온다\n",
    "        ## 어떤 coulmn을 label로 사용할 것인지 label_column을 통해 지정해주어야 한다. \n",
    "        raw_data.rename(lambda x : x.split('.')[-1], axis='columns', inplace = True)\n",
    "        label_column = label_column.split('.')[-1]\n",
    "        self.file_list = list(raw_data['file_id'])\n",
    "        self.label_list = list(raw_data[label_column])\n",
    "        self.label_dict = {}\n",
    "        for idx, label in enumerate(set(self.label_list)):\n",
    "            self.label_dict[label] = idx\n",
    "        \n",
    "        self.gene_list = gene_list\n",
    "        self.normalized = normalized\n",
    "        \n",
    "            \n",
    "       \n",
    "    def __iter__(self):\n",
    "         for filename,label in zip(self.file_list, self.label_list):\n",
    "            logger.info('load_file')\n",
    "            raw_data = pd.read_csv(\"./tcga/\"+filename+\"/\"+os.listdir(\"./tcga/\"+filename)[0], sep = '\\t')\n",
    "            # mount된 디렉토리에서 manifest 목록에 있는 file을 순차적으로 불러온다\n",
    "            label_idx = self.label_dict[label]\n",
    "            gene_column = ''\n",
    "            count_column = ''\n",
    "            if 'gene_id' in list(raw_data.columns):  gene_column = 'gene_id'\n",
    "            elif 'gene' in list(raw_data.columns):  gene_column = 'gene'\n",
    "                \n",
    "            if not self.normalized:\n",
    "                if 'raw_count' in list(raw_data.columns):  count_column = 'raw_count'\n",
    "                elif 'raw_counts' in list(raw_data.columns):  count_column = 'raw_counts'\n",
    "            #raw count도 파일마다 컬럼명이 다름.......        \n",
    "            \n",
    "            else : count_column = 'normalized_count'\n",
    "            #print(raw_data.columns)\n",
    "            try:\n",
    "                raw_data[gene_column] = raw_data[gene_column].apply(lambda x: x.split('|')[0])\n",
    "                #gene_id는 [symbol|PubMedID] (예 BRCA1|11331580) 의 형식으로 기재되어 있음\n",
    "                #Symbol만을 남김\n",
    "                raw_data = raw_data[raw_data[gene_column].isin(self.gene_list)]\n",
    "                expression = raw_data[count_column].values\n",
    "                logger.info('yield')\n",
    "                yield (torch.tensor(expression) ,label_idx)\n",
    "            except Exception as e:\n",
    "                logger.info('skip')\n",
    "                #print(e)\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Caching Strategy\n",
    "\n",
    "맨 처음으로 파일을 불러올 땐 약 20000여개의 유전자에 대한 발현량이 있는 파일을 통채로 불러온다.\n",
    "\n",
    "gene list에 존재하는 유전자 : 1385개\n",
    "\n",
    "모종의 이유로 gene_list에 존재하는 유전자 중 데이터에 존재하는 유전자 1346개\n",
    "\n",
    "실제로 사용하는 데이터는 지정한 gene list에 있는 유전자(본 예시에서는 1346개)의 발현량이므로\n",
    "이 1346개의 데이터만 캐싱하여 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "raw_data, label_column = manifest_loader(fields, filters,\"cases.disease_type\")\n",
    "train_data, test_data = train_test_split(raw_data, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmsori/.conda/envs/nkhugh/lib/python3.8/site-packages/pandas/core/frame.py:4290: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().rename(\n"
     ]
    }
   ],
   "source": [
    "train_dataset = GeneExpressionLoader(train_data, label_column,gene_list)\n",
    "test_dataset = GeneExpressionLoader(train_data, label_column,gene_list)\n",
    "train_loader = DataLoader(train_dataset, batch_size = 10)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test split 에 관해\n",
    "\n",
    "하나의 manifest file 목록을 split하여 train과 test set을 생성함.\n",
    "\n",
    "실제로는 data loader 안에서 유효한 데이터인지 판별하므로 실제 train과 test의 비중이 다름. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for expression, label in train_loader:\n",
    "    print(expression)\n",
    "    i +=1\n",
    "    if i == 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "모델 자체는 간단함.\n",
    "\n",
    "간단한 모델 구조 안에서 complexity를 높이기 위해 층을 깊게 쌓음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiseaseClassification(nn.Module):\n",
    "    def __init__(self, input_dim ,hidden_dim, output_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim,500)\n",
    "        self.fc2 = nn.Linear(500,1000)\n",
    "        self.fc3 = nn.Linear(1000,2000)\n",
    "        self.fc4 = nn.Linear(2000,1000)\n",
    "        self.fc5 = nn.Linear(1000,500)\n",
    "        self.fc6 = nn.Linear(500, output_dim)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x.float()\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 1346\n",
    "OUTPUT_SIZE = 3\n",
    "HIDDEN_SIZE = 500\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = DiseaseClassification(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True) \n",
    "    correct = max_preds.squeeze(1).eq(y)\n",
    "    return correct.sum() / torch.FloatTensor([y.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    \n",
    "    for expression, label in iterator:\n",
    "        \n",
    "        logger.info('start_batch_train')\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predictions = model(expression)\n",
    "\n",
    "        loss = criterion(predictions, label)\n",
    "                \n",
    "        acc = categorical_accuracy(predictions, label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        logger.info('end_batch_train')\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for expression, label in iterator:\n",
    "\n",
    "            predictions = model(expression)\n",
    "        \n",
    "            loss = criterion(predictions, label)  \n",
    "            acc = categorical_accuracy(predictions, label)\n",
    "        \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr = 0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 관찰\n",
    "\n",
    "하나의 batch를 train하는 시간은 의외로 길지 않음(모델이 간단해서?)\n",
    "\n",
    "오히려 dataloader streaming하는 overhead가 크다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 10\n",
    "import time\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, train_loader, optimizer, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nkhugh-anaconda",
   "language": "python",
   "name": "nkhugh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
